{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d61ed0d-783d-45f7-99a2-f3c87dbccfc2",
   "metadata": {},
   "source": [
    "# Predictive Algorithms\n",
    "\n",
    "This notebook demonstrates the development and evaluation of predictive algorithms for survival analysis. The aim is to train, evaluate, and optimize various survival models to predict overall survival (OS) and progression-free survival (PFS) for patients, using both classical and deep learning techniques. It includes data preprocessing, model training, hyperparameter optimization, and visualization of survival curves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00789d13-178f-42bd-81a8-947b666139f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295f5b2e-bc29-4325-8a49-63bac2552dc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.data.data_processing import DataSplitter, DataPreprocessor\n",
    "from src.data.lookups import LookupManager\n",
    "\n",
    "db_config_path = '/home/jupyter/.my.cnf'\n",
    "db_name = 'actin_personalization'\n",
    "query = \"SELECT * FROM knownPalliativeTreatments\"\n",
    "\n",
    "preprocessor = DataPreprocessor(db_config_path, db_name)\n",
    "\n",
    "lookup_manager = LookupManager()\n",
    "features = lookup_manager.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d87935-155b-40c6-8edc-d7c8326d2ef5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "In this section, we set up the data pipeline for survival analysis. The `DataSplitter` and `DataPreprocessor` classes are used to load, preprocess, and split the data into training and testing sets. This ensures the survival data is structured appropriately for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a8049d-485b-4da4-913f-510906547b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(query, event_col, duration_col, features):\n",
    "    splitter = DataSplitter(test_size=0.1, random_state=42)\n",
    "    \n",
    "    df, features, encoded_columns = preprocessor.preprocess_data(query, duration_col, event_col, features)\n",
    "                          \n",
    "    y = Surv.from_dataframe(event=event_col, time=duration_col, data=df)\n",
    "    X_train, X_test, y_train, y_test = splitter.split(df[features], df, 'systemicTreatmentPlan', encoded_columns)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, encoded_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f504d62f-a330-402b-b58e-a3b3f74a9658",
   "metadata": {},
   "source": [
    "## Train and Evaluate Models\n",
    "\n",
    "This section defines the function `train_evaluate_models`, which trains various survival models using predefined configurations. The trained models are evaluated using the following metrics:\n",
    "\n",
    "- **C-Index**: The Concordance Index measures how well the predicted survival times align with the actual outcomes. It is a measure of discrimination, indicating the model's ability to correctly rank the survival times of patients. A higher value indicates better predictive accuracy.\n",
    "\n",
    "- **Integrated Brier Score (IBS)**: This metric evaluates the accuracy of the survival probability predictions over time. It combines the squared differences between predicted and actual survival probabilities, weighted by the survival distribution. Lower values indicate better predictive performance.\n",
    "\n",
    "- **Calibration Error (CE)**: Calibration error assesses how well the predicted survival probabilities match the observed probabilities. It indicates whether the model is systematically overestimating or underestimating survival probabilities. Lower values signify better calibration.\n",
    "\n",
    "- **Area Under the Curve (AUC)**: For survival models, AUC is typically computed over a time-dependent ROC curve, reflecting the model's discrimination ability at different time points. Higher AUC values indicate better overall performance.\n",
    "\n",
    "Together, these metrics provide a comprehensive evaluation of the models' predictive performance, capturing different aspects of accuracy, discrimination, and calibration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71536d40-de44-4f55-9e44-c1f1e3cc5fc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_survival_curves_for_patient(trained_models, X_test, y_test, patient_index, event_col, duration_col, plot_title):\n",
    "    \"\"\"\n",
    "    Plot survival curves for a specific patient using trained models.\n",
    "    \"\"\"\n",
    "    X_patient = X_test.iloc[[patient_index]]\n",
    "    actual_duration_days = y_test[duration_col].iloc[patient_index]\n",
    "    actual_event = y_test[event_col].iloc[patient_index]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for model_name, model in trained_models.items():\n",
    "        try:\n",
    "            surv_funcs = model.predict_survival_function(X_patient)\n",
    "\n",
    "            times = np.linspace( max(fn.x[0] for fn in surv_funcs), min(fn.x[-1] for fn in surv_funcs), 100)\n",
    "            surv_probs = np.row_stack([fn(times) for fn in surv_funcs])\n",
    "            \n",
    "            plt.step(times / 30.44, surv_probs[0], where=\"post\", label=model_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting survival curves for model {model_name}: {e}\")\n",
    "\n",
    "    marker_color = 'red' if actual_event else 'blue'\n",
    "    marker_label = \"Event Time\" if actual_event else \"Censoring Time\"\n",
    "    plt.axvline(x=actual_duration_days / 30.44, color=marker_color, linestyle='--', label=marker_label)\n",
    "\n",
    "    plt.title(f\"Predicted {plot_title} Survival Curves\")\n",
    "    plt.xlabel(\"Time (months)\")\n",
    "    plt.ylabel(\"Survival Probability\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9317cc-59e1-430f-8e93-d3e10257d308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.models.survival_models import *\n",
    "from src.models.model_trainer import *\n",
    "from sksurv.util import Surv\n",
    "\n",
    "def train_evaluate_models(query, event_col, duration_col, features, configs, patient_index = 78, plot_title = \"Overall\"):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, encoded_columns = get_data(query, event_col, duration_col, features)\n",
    "        \n",
    "    for model_name, params in configs.items():\n",
    "        if 'input_size' in params:\n",
    "            params['input_size'] = X_train.shape[1]\n",
    "\n",
    "    models = {\n",
    "        'DeepSurv': DeepSurv(**configs['DeepSurv']),\n",
    "        'LogisticHazardModel': LogisticHazardModel(**configs['LogisticHazardModel']),\n",
    "        'DeepHitModel': DeepHitModel(**configs['DeepHitModel']),\n",
    "        'PCHazardModel': PCHazardModel(**configs['PCHazardModel']),\n",
    "        'MTLRModel': MTLRModel(**configs['MTLRModel']),\n",
    "        'AalenAdditive': AalenAdditiveModel(**configs['AalenAdditive']),\n",
    "        'CoxPH': CoxPHModel(**configs['CoxPH']),\n",
    "        'RandomSurvivalForest': RandomSurvivalForestModel(**configs['RandomSurvivalForest']),\n",
    "        'GradientBoosting': GradientBoostingSurvivalModel(**configs['GradientBoosting']),\n",
    "    }\n",
    "    \n",
    "    trainer = ModelTrainer(models=models, n_splits=5, random_state=42)\n",
    "\n",
    "    results, trained_models = trainer.train_and_evaluate(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        treatment_col='systemicTreatmentPlan',\n",
    "        encoded_columns=encoded_columns,\n",
    "        event_col= event_col,\n",
    "        duration_col= duration_col\n",
    "    )\n",
    "    \n",
    "    results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "    results_df.reset_index(inplace=True)\n",
    "    results_df.rename(columns={'index': 'Model'}, inplace=True)\n",
    "    \n",
    "    plot_survival_curves_for_patient(trained_models, X_test, y_test, patient_index, event_col, duration_col, plot_title)\n",
    "   \n",
    "    return results_df, trained_models\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32032079-2e45-443b-ad67-1721f39c2c4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Best Model Configurations\n",
    "\n",
    "The best configurations for OS and PFS models were determined using hyperparameter optimization, as defined below in this notebook. These configurations are used to instantiate the models for training and evaluation. Below are the best configurations for the OS and PFS models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec98f3c-cdf9-46a4-ad7b-797f97bca980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best configurations for OS\n",
    "os_configs = {\n",
    "    'DeepSurv': {\n",
    "        'input_size': None, \n",
    "        'num_nodes': [256, 128, 64],\n",
    "        'batch_norm': False,\n",
    "        'dropout': 0.1,\n",
    "        'weight_decay': 0.0001,\n",
    "        'lr': 0.001,\n",
    "        'activation': 'relu',\n",
    "        'optimizer': 'RMSprop',\n",
    "        'batch_size': 32,\n",
    "        'epochs': 500,\n",
    "    },\n",
    "    'LogisticHazardModel': {\n",
    "        'input_size': None, \n",
    "        'num_nodes': [256, 128, 64],\n",
    "        'lr': 0.0005,\n",
    "        'dropout': 0.15,\n",
    "        'batch_size': 128,\n",
    "        'epochs': 500,\n",
    "        'num_durations': 60,\n",
    "        'early_stopping_patience': 50,\n",
    "        'optimizer': 'RMSprop',\n",
    "    },\n",
    "    'DeepHitModel': {\n",
    "        'input_size': None, \n",
    "        'num_nodes': [64, 32],\n",
    "        'activation': 'elu',\n",
    "        'alpha': 0.2,\n",
    "        'sigma': 0.1,\n",
    "        'weight_decay': 0.0001,\n",
    "        'optimizer': 'Adam',\n",
    "        'dropout': 0.1,\n",
    "        'lr': 1e-3,\n",
    "        'batch_size': 32,\n",
    "        'epochs': 500,\n",
    "        'num_durations': 60,\n",
    "    },\n",
    "    'PCHazardModel': {\n",
    "        'input_size': None,  \n",
    "        'num_nodes': [256, 128, 64],\n",
    "        'batch_norm': True,\n",
    "        'dropout': 0.15,\n",
    "        'lr': 0.01,\n",
    "        'batch_size': 32,\n",
    "        'epochs': 500,\n",
    "        'num_durations': 120,\n",
    "        'optimizer': 'Adam',\n",
    "    },\n",
    "    'AalenAdditive': {\n",
    "        'fit_intercept': True,\n",
    "        'alpha': 0.05,\n",
    "        'coef_penalizer': 10.0,\n",
    "        'smoothing_penalizer': 0.0,\n",
    "    },\n",
    "    'CoxPH': {\n",
    "        'alpha': 1.0,\n",
    "        'ties': 'breslow',\n",
    "        'n_iter': 100,\n",
    "        'tol': 1e-7,\n",
    "    },\n",
    "    'RandomSurvivalForest': {\n",
    "        'n_estimators': 200,\n",
    "        'max_depth': 50,\n",
    "        'min_samples_split': 50,\n",
    "        'min_samples_leaf': 5,\n",
    "        'max_features': None,\n",
    "        'random_state': 42,\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'learning_rate': 0.05, \n",
    "        'n_estimators': 100, \n",
    "        'max_depth': 10, \n",
    "        'subsample': 1.0, \n",
    "        'min_samples_leaf': 20, \n",
    "        'min_samples_split': 10, \n",
    "        'max_features': 'log2'\n",
    "    },\n",
    "}\n",
    "\n",
    "# PFS is still running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41b414d-38cb-4670-8611-5efe71f73e03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os_model_outcomes, os_trained_models =  train_evaluate_models(query, event_col='hadSurvivalEvent', duration_col='observedOsFromTreatmentStartDays', features=features, configs=os_configs)\n",
    "os_model_outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab611cb-e5de-4c00-abae-9fbf35c6ceb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pfs_model_outcomes, pfs_trained_models = train_evaluate_models(query, event_col='hadProgressionEvent', duration_col='observedPfsDays', features=features, configs=pfs_configs)\n",
    "pfs_model_outcomes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb1bbf47-8e6b-4f7c-94b8-2f46e839e70c",
   "metadata": {},
   "source": [
    "OS:\n",
    "With standardization:\n",
    "- LogisticHazardModel - C-Index: 0.5000, IBS: 0.1386, CE: 0.2092, AUC: 0.5003\n",
    "- DeepHitModel - C-Index: 0.5104, IBS: 0.2534, CE: 0.3382, AUC: 0.5145\n",
    "- PCHazardModel - C-Index: 0.5000, IBS: 0.1268, CE: 0.1813, AUC: 0.5003\n",
    "- MTLRModel - C-Index: 0.5283, IBS: 0.2213, CE: 0.2643, AUC: 0.5363\n",
    "- DeepSurv - C-Index: 0.5144, IBS: 0.0876, CE: 0.0670, AUC: 0.5268\n",
    "- AalenAdditive - C-Index: 0.6158, IBS: 1.7504, CE: 0.0458, AUC: 0.7030\n",
    "- CoxPH - C-Index: 0.6474, IBS: 0.0746, CE: 0.0419, AUC: 0.7065\n",
    "- RandomSurvivalForest - C-Index: 0.6441, IBS: 0.0804, CE: 0.0497, AUC: 0.6492\n",
    "- GradientBoosting - C-Index: 0.6605, IBS: 0.0730, CE: 0.0555, AUC: 0.7260\n",
    "\n",
    "PFS:\n",
    "- DeepSurv - C-Index: 0.6199, IBS: 0.0696, CE: 0.1257, AUC: 0.6793\n",
    "- LogisticHazardModel - C-Index: 0.5776, IBS: 0.0691, CE: 0.1387, AUC: 0.6595\n",
    "- DeepHitModel - C-Index: 0.5032, IBS: 0.0740, CE: 0.0921, AUC: 0.4994\n",
    "- PCHazardModel - C-Index: 0.5761, IBS: 0.0689, CE: 0.1366, AUC: 0.6674\n",
    "- MTLRModel - C-Index: 0.5594, IBS: 0.0712, CE: 0.1309, AUC: 0.5937\n",
    "- AalenAdditive - C-Index: 0.6287, IBS: 3.7058, CE: 0.0640, AUC: 0.6944\n",
    "- CoxPH - C-Index: 0.6372, IBS: 0.0679, CE: 0.1214, AUC: 0.6986\n",
    "- RandomSurvivalForest - C-Index: 0.6355, IBS: 0.0717, CE: 0.1103, AUC: 0.6305\n",
    "- GradientBoosting - C-Index: 0.6444, IBS: 0.0677, CE: 0.1313, AUC: 0.7012"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f3ce52-cd89-4fb4-89fa-fc8def4cf880",
   "metadata": {},
   "source": [
    "### Metric comparison: OS vs. PFS\n",
    "\n",
    "This section visualizes the comparison of model performance metrics (C-Index, IBS, CE, AUC) for OS and PFS. The bar plots highlight the strengths and weaknesses of each model in the two prediction tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c109ba-8bd2-47a1-8ade-3b63cbcb14b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_all_metrics(pfs_df, os_df):\n",
    "    pfs_df['Type'] = 'PFS'\n",
    "    os_df['Type'] = 'OS'\n",
    "\n",
    "    combined_df = pd.concat([pfs_df, os_df], ignore_index=True)\n",
    "\n",
    "    metrics = ['c_index', 'ibs', 'ce', 'auc']\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i]\n",
    "        sns.barplot(\n",
    "            x='Model',\n",
    "            y=metric,\n",
    "            hue='Type',\n",
    "            data=combined_df,\n",
    "            ax=ax,\n",
    "            palette='Set1'\n",
    "        )\n",
    "        ax.set_title(f'{metric.upper()} Comparison: OS vs. PFS')\n",
    "        ax.set_xlabel('Model')\n",
    "        ax.set_ylabel(metric.upper())\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "        ax.legend(title='Type', loc='best')\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f799e6a-4946-4187-b087-dde792f84752",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_all_metrics(pfs_model_outcomes, os_model_outcomes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cfc2de-8126-4ccc-ae60-b846528673a4",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization\n",
    "\n",
    "Hyperparameter optimization is performed for each model using a defined grid of parameters. The `random_parameter_search` function samples configurations to identify the optimal parameters for each model. This ensures that models achieve their best performance for the given data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220d8f00-01f1-43d4-bfc9-969e6302dfb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'LogisticHazardModel': [\n",
    "        {\n",
    "            'num_nodes': [[256, 128, 64], [128, 64, 32],[128, 64], [64, 32], [64], [32]],\n",
    "            'lr': [0.001, 0.0005, 0.01],\n",
    "            'dropout': [0.1, 0.15, 0.2],\n",
    "            'batch_size': [32, 64, 128],\n",
    "            'patience': [20, 30, 50],\n",
    "            'optimizer': ['Adam', 'RMSprop']\n",
    "        }\n",
    "    ],\n",
    "    'DeepHitModel': [\n",
    "        {\n",
    "            'num_nodes': [[256, 128, 64], [128, 64, 32],[128, 64], [64, 32], [64], [32]],\n",
    "            'activation': ['swish', 'elu', 'relu'],\n",
    "            'alpha': [0.2, 0.3, 0.4],\n",
    "            'sigma': [0.05, 0.1, 0.2],\n",
    "            'weight_decay': [1e-3, 5e-4, 1e-4],\n",
    "            'optimizer': ['Adam', 'RMSprop']\n",
    "        }\n",
    "    ],\n",
    "    'PCHazardModel': [\n",
    "        {\n",
    "            'num_nodes': [[256, 128, 64],[128, 64, 32], [128, 64], [64, 32], [64], [32]],\n",
    "            'num_durations': [60, 80, 100, 120],\n",
    "            'lr': [0.0005, 0.001, 0.01],\n",
    "            'dropout': [0.1, 0.15, 0.2],\n",
    "            'optimizer': ['Adam', 'RMSprop']\n",
    "        }\n",
    "    ],\n",
    "    'MTLRModel': [\n",
    "        {\n",
    "            'num_nodes': [[256, 128, 64],[128, 64, 32], [128, 64], [64, 32], [64], [32]],\n",
    "            'num_durations': [80, 100, 120],\n",
    "            'lr': [0.0005, 0.001, 0.01],\n",
    "            'dropout': [0.1, 0.15, 0.2],\n",
    "            'optimizer': ['Adam', 'RMSprop']\n",
    "        }\n",
    "    ],\n",
    "    'AalenAdditive': [\n",
    "        {\n",
    "            'fit_intercept': [True, False],\n",
    "            'alpha': [0.01, 0.05, 0.1],\n",
    "            'coef_penalizer': [0.5, 1.0, 2.0, 5.0, 10.0],\n",
    "            'smoothing_penalizer': [0.0, 0.5, 1.0],\n",
    "        }\n",
    "    ],\n",
    "    'CoxPH': [\n",
    "        {\n",
    "            'alpha': [0, 0.05, 0.1, 0.2, 0.5, 1.0],\n",
    "            'ties': ['breslow', 'efron'],\n",
    "            'n_iter': [100, 200, 500],\n",
    "            'tol': [1e-5, 1e-7, 1e-9],\n",
    "        }\n",
    "    ],\n",
    "    'RandomSurvivalForest': [\n",
    "        {\n",
    "            'n_estimators': [50, 100, 200, 500],\n",
    "            'max_depth': [5, 10, 20, 50],\n",
    "            'min_samples_split': [10, 20, 50, 100],\n",
    "            'min_samples_leaf': [5, 10, 15, 20, 30],\n",
    "            'max_features': ['sqrt', 'log2', None]\n",
    "        }\n",
    "    ],\n",
    "    'GradientBoosting': [\n",
    "        {\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'n_estimators': [50, 100, 200, 300, 500],\n",
    "            'max_depth': [3, 5, 10],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'min_samples_leaf': [5, 10, 20],\n",
    "            'min_samples_split': [10, 20, 50],\n",
    "            'max_features': ['sqrt', 'log2', None]\n",
    "        }\n",
    "    ],\n",
    "    'DeepSurv': [\n",
    "        {\n",
    "            'num_nodes': [[256, 128, 64], [128, 64, 32], [128, 64], [64, 32], [64], [32]],\n",
    "            'batch_norm': [True, False],\n",
    "            'dropout': [0.1, 0.2, 0.3],\n",
    "            'weight_decay': [1e-3, 5e-4, 1e-4],\n",
    "            'lr': [0.001, 0.0005, 0.005, 0.01],\n",
    "            'activation': ['elu', 'relu'],\n",
    "            'optimizer': ['Adam', 'RMSprop']\n",
    "        }\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050389e7-8a68-4638-abbe-d076ac4ee7ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import product\n",
    "\n",
    "def random_parameter_search(param_dict, n_samples):\n",
    "    \"\"\"\n",
    "    Randomly sample `n_samples` parameter combinations from the given param_dict.\n",
    "    param_dict should be a dict of lists, e.g.:\n",
    "      {\n",
    "        'lr': [0.001, 0.0005, 0.01],\n",
    "        'dropout': [0.1, 0.2],\n",
    "      }\n",
    "    \"\"\"\n",
    "    keys = list(param_dict.keys())\n",
    "    values = [param_dict[k] for k in keys]\n",
    "\n",
    "    all_combos = list(product(*values))\n",
    "    if len(all_combos) <= n_samples:\n",
    "        return [dict(zip(keys, combo)) for combo in all_combos]\n",
    "\n",
    "    sampled_combos = random.sample(all_combos, n_samples)\n",
    "    return [dict(zip(keys, combo)) for combo in sampled_combos]\n",
    "\n",
    "def hyperparameter_search(\n",
    "    X_train, y_train, X_test, y_test, treatment_col, encoded_columns, event_col, duration_col,\n",
    "    base_models, param_grids, n_samples=20, random_state=42\n",
    "):\n",
    "    random.seed(random_state)\n",
    "    best_models = {}\n",
    "    all_results = {}\n",
    "    trainer = ModelTrainer(models={}, n_splits=5, random_state=random_state)\n",
    "\n",
    "    for model_name, model_instance in base_models.items():\n",
    "        model_class = type(model_instance)\n",
    "        if model_name not in param_grids:\n",
    "            print(f\"No hyperparameter grid found for {model_name}, skipping optimization...\")\n",
    "            best_models[model_name] = (model_instance, None)\n",
    "            continue\n",
    "\n",
    "        best_score = -np.inf\n",
    "        best_params = None\n",
    "        best_model_trained = None\n",
    "        all_results[model_name] = []\n",
    "\n",
    "        for param_dict in param_grids[model_name]:\n",
    "            sampled_params = random_parameter_search(param_dict, n_samples)\n",
    "          \n",
    "            for params in sampled_params:\n",
    "                if issubclass(model_class, NNSurvivalModel):\n",
    "                    new_model = model_class(input_size=X_train.shape[1], **params)\n",
    "                else:\n",
    "                    new_model = model_class(**params)\n",
    "\n",
    "                print(f\"Training {model_name} with parameters: {params}\")\n",
    "\n",
    "                # Use the new_model here instead of model_instance\n",
    "                trainer.models = {model_name: new_model}\n",
    "                results, trained_models = trainer.train_and_evaluate(\n",
    "                    X_train, y_train, X_test, y_test,\n",
    "                    treatment_col=treatment_col,\n",
    "                    encoded_columns=encoded_columns,\n",
    "                    event_col=event_col,\n",
    "                    duration_col=duration_col\n",
    "                )\n",
    "\n",
    "                current_score = results[model_name]['auc']\n",
    "                all_results[model_name].append((params, results[model_name]))\n",
    "\n",
    "                if current_score > best_score:\n",
    "                    best_score = current_score\n",
    "                    best_params = params\n",
    "                    best_model_trained = trained_models[model_name]\n",
    "\n",
    "\n",
    "        best_models[model_name] = (best_model_trained, best_params)\n",
    "        print(f\"Best params for {model_name}: {best_params} with auc={best_score}\")\n",
    "\n",
    "    return best_models, all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c208e0c-00eb-47dd-a74b-bf3e021cb4da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_hyperparameters(query, event_col, duration_col, features):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, encoded_columns = get_data(query, event_col, duration_col, features)\n",
    "          \n",
    "    models = {\n",
    "        'DeepSurv': DeepSurv(input_size=X_train.shape[1]),\n",
    "        'LogisticHazardModel': LogisticHazardModel(input_size=X_train.shape[1]),\n",
    "        'DeepHitModel': DeepHitModel(input_size=X_train.shape[1]), \n",
    "        'PCHazardModel': PCHazardModel(input_size=X_train.shape[1]), \n",
    "        'MTLRModel': MTLRModel(input_size=X_train.shape[1]),\n",
    "        'AalenAdditive': AalenAdditiveModel(),\n",
    "        'CoxPH': CoxPHModel(),\n",
    "        'RandomSurvivalForest': RandomSurvivalForestModel(),\n",
    "        'GradientBoosting': GradientBoostingSurvivalModel(),\n",
    "    }\n",
    "    \n",
    "    best_models, all_results = hyperparameter_search(\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        treatment_col='systemicTreatmentPlan', encoded_columns=encoded_columns,\n",
    "        event_col=event_col, duration_col=duration_col,\n",
    "        base_models=models, param_grids=param_grids, n_samples = 20\n",
    "    )\n",
    "    \n",
    "    for model_name, trials in all_results.items():\n",
    "        for params, metrics in trials:\n",
    "            print(model_name, params, metrics['c_index'], metrics['ibs'], metrics['auc'], metrics['ce'])\n",
    "            \n",
    "    return best_models, all_results\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf478771-030d-41ed-a26e-f91ac670995f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os_best_models, os_results = optimize_hyperparameters(query, event_col = 'hadSurvivalEvent', duration_col = 'observedOsFromTreatmentStartDays', features = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b83205-fa98-4c80-abeb-32d149528523",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfs_best_models, pfs_results = optimize_hyperparameters(query, event_col = 'hadProgressionEvent', duration_col = 'observedPfsDays', features=features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (prediction_env)",
   "language": "python",
   "name": "prediction_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

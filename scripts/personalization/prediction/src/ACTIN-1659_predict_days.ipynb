{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d61ed0d-783d-45f7-99a2-f3c87dbccfc2",
   "metadata": {},
   "source": [
    "# Predictive Algorithms for Survival Analysis for predicting survival days\n",
    "\n",
    "This notebook demonstrates a full pipeline for training and evaluating predictive models for survival analysis. We handle both overall survival (OS) and progression‑free survival (PFS) by reusing the same functions. The pipeline loads and preprocesses data, visualizes the target distributions (before and after log transformation), displays correlation heatmaps and residual plots, evaluates a set of regression models, and performs hyperparameter optimization (using random search) for selected models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00789d13-178f-42bd-81a8-947b666139f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bbff7b-ecbb-418e-83c5-84b5d19c1e48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/data/repos/actin-personalization/scripts/personalization/prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b78c7cb-fe63-46a0-9873-be7edde348b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.data.data_processing import DataSplitter, DataPreprocessor\n",
    "from src.data.lookups import LookupManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295f5b2e-bc29-4325-8a49-63bac2552dc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_config_path = '/home/jupyter/.my.cnf'\n",
    "db_name = 'actin_personalization'\n",
    "query = \"SELECT * FROM knownPalliativeTreatments\"\n",
    "\n",
    "preprocessor = DataPreprocessor(db_config_path, db_name)\n",
    "\n",
    "lookup_manager = LookupManager()\n",
    "features = lookup_manager.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d87935-155b-40c6-8edc-d7c8326d2ef5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We define a function to load and prepare the data. We load the data, filter for rows with the event of interest, apply a log transformation to the survival days target, and splits the features and target.\n",
    "\n",
    "TODO write something about why drop censored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a8049d-485b-4da4-913f-510906547b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(query, event_col, duration_col, features, group_treatments):\n",
    "    splitter = DataSplitter(test_size=0.1, random_state=42)\n",
    "    \n",
    "    df, features, encoded_columns = preprocessor.preprocess_data(query, duration_col, event_col, features, group_treatments)\n",
    "    \n",
    "    df = df[df[event_col] == 1]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = splitter.split(df[features], df[duration_col], encoded_columns = encoded_columns)\n",
    "    \n",
    "    return df, X_train, X_test, y_train, y_test, encoded_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfbf72a-d031-43fe-ae55-812bb0cbef2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os_df, os_X_train, os_X_test, os_y_train, os_y_test, os_encoded_columns = get_data(\n",
    "    query, 'hadSurvivalEvent', 'observedOsFromTreatmentStartDays', features, group_treatments=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93219473-6d01-4c4b-a626-e7cca02008fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pfs_df, pfs_X_train, pfs_X_test, pfs_y_train, pfs_y_test, pfs_encoded_columns = get_data(\n",
    "    query, 'hadProgressionEvent', 'observedPfsDays', features, group_treatments=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4020806d-93fd-4f19-b959-82123678024e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualization Functions\n",
    "\n",
    "We now visualize the distribution of the OS target before and after log transformation. This helps us understand the skewness of the data and the effect of the transformation. These functions also generate a correlation heatmap and plot residuals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb0d7a3-27d7-4749-b6cc-86bce10e978a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def visualize_target_distribution(df, target_col):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.histplot(df[target_col], kde=True, bins=30)\n",
    "    plt.title(f\"Distribution of {target_col} (Days)\")\n",
    "    plt.xlabel(f\"{target_col} (Days)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "    log_target = np.log1p(df[target_col])\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.histplot(log_target, kde=True, bins=30, color='green')\n",
    "    plt.title(f\"Distribution of Log-Transformed {target_col}\")\n",
    "    plt.xlabel(f\"Log({target_col} + 1)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "    return log_target\n",
    "\n",
    "def plot_correlation_heatmap(df, target_col):\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    for col in [target_col, 'hadSurvivalEvent', 'hadProgressionEvent']:\n",
    "        if col in numeric_cols:\n",
    "            numeric_cols.remove(col)\n",
    "    corr_matrix = df[numeric_cols + [target_col]].corr()\n",
    "    plt.figure(figsize=(12,10))\n",
    "    sns.heatmap(corr_matrix, annot=False, fmt=\".2f\", cmap=\"coolwarm\", square=True)\n",
    "    plt.title(f\"Correlation Heatmap with {target_col}\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_residuals(y_true, y_pred):\n",
    "    residuals = y_true - y_pred\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.histplot(residuals, kde=True, bins=30)\n",
    "    plt.title(\"Residuals Distribution (Log Scale)\")\n",
    "    plt.xlabel(\"Residuals\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.scatterplot(x=y_pred, y=residuals)\n",
    "    plt.axhline(0, color='red', linestyle='--')\n",
    "    plt.title(\"Residuals vs Predicted (Log Scale)\")\n",
    "    plt.xlabel(\"Predicted Log Value\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_predictions(y_test, y_pred, title):\n",
    "    y_test_orig = np.expm1(y_test)\n",
    "    y_pred_orig = np.expm1(y_pred)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.scatterplot(x=y_test_orig, y=y_pred_orig)\n",
    "    plt.plot([y_test_orig.min(), y_test_orig.max()], [y_test_orig.min(), y_test_orig.max()], color='red', lw=2)\n",
    "    plt.xlabel(\"Actual (Days)\")\n",
    "    plt.ylabel(\"Predicted (Days)\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2db9fad-9150-4734-9c01-15bc0cd36957",
   "metadata": {},
   "source": [
    "## Model Evaluation and Optimization Functions\n",
    "\n",
    "The `evaluate_models` function trains a set of models on the log-transformed target and reports performance on both the log scale and the original scale. The `optimize_model_random_search` function performs randomized hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6eafd2-94ea-455a-bd6a-5f6e58d1070c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a07c23b-7683-4f70-8420-05b8325f5029",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_models(models, X_train, X_test, y_train, y_test):\n",
    "    results = []\n",
    "    for model_name, model in models.items():\n",
    "        y_train_log = np.log1p(y_train)\n",
    "        y_test_log = np.log1p(y_test)\n",
    "        model.fit(X_train, y_train_log)\n",
    "        y_pred_log = model.predict(X_test)\n",
    "        y_pred_orig = np.expm1(y_pred_log)\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"MSE (log)\": mean_squared_error(y_test_log, y_pred_log),\n",
    "            \"R² (log)\": r2_score(y_test_log, y_pred_log),\n",
    "            \"MSE (original)\": mean_squared_error(y_test, y_pred_orig),\n",
    "            \"R² (original)\": r2_score(y_test, y_pred_orig)\n",
    "        })\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd7d9c7-79df-43b0-a434-ec393115844d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hyperparameter_search(model, param_dist, X_train, y_train, cv_folds=5, n_iter=20):\n",
    "    random_search = RandomizedSearchCV(model, param_dist, n_iter=n_iter, cv=cv_folds,\n",
    "                                       scoring='neg_mean_squared_error', verbose=1,\n",
    "                                       n_jobs=-1, random_state=42)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    return random_search.best_estimator_, random_search.best_params_, random_search.best_score_\n",
    "\n",
    "def optimize_model_random_search(X_train, y_train):\n",
    "    rf_param_dist = {\n",
    "        'n_estimators': [100, 200, 300, 500],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['auto', 'sqrt', 'log2']\n",
    "    }\n",
    "    best_rf, best_rf_params, best_rf_score = hyperparameter_search(RandomForestRegressor(random_state=42), rf_param_dist, X_train, np.log1p(y_train))\n",
    "    print(\"RandomForest best parameters:\", best_rf_params)\n",
    "    print(\"RandomForest best score (neg MSE):\", best_rf_score)\n",
    "    \n",
    "    gb_param_dist = {\n",
    "        'n_estimators': [100, 200, 300, 500],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 4, 5, 6],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['auto', 'sqrt', 'log2']\n",
    "    }\n",
    "    best_gb, best_gb_params, best_gb_score = hyperparameter_search(GradientBoostingRegressor(random_state=42), gb_param_dist, X_train, np.log1p(y_train))\n",
    "    print(\"GradientBoosting best parameters:\", best_gb_params)\n",
    "    print(\"GradientBoosting best score (neg MSE):\", best_gb_score)\n",
    "    \n",
    "    mlp_param_dist = {\n",
    "        'hidden_layer_sizes': [(64, 32), (128, 64, 32), (32,), (128, 64, 64, 32), (32, 32)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'solver': ['adam', 'lbfgs'],\n",
    "        'alpha': [1e-4, 1e-3, 1e-2],\n",
    "        'learning_rate_init': [0.001, 0.01, 0.05]\n",
    "    }\n",
    "    best_mlp, best_mlp_params, best_mlp_score = hyperparameter_search(MLPRegressor(max_iter=1000, random_state=42), mlp_param_dist, X_train, np.log1p(y_train))\n",
    "    print(\"MLPRegressor best parameters:\", best_mlp_params)\n",
    "    print(\"MLPRegressor best score (neg MSE):\", best_mlp_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceeb9e4-fb25-4ff1-8fc6-59924e63dde1",
   "metadata": {},
   "source": [
    "## KNN and Best-K Determination\n",
    "\n",
    "The function `determine_best_k_nn` runs cross‑validation over a range of K values for a K‑Nearest Neighbors regressor (using log‑transformed target values) and plots the cross‑validated negative MSE. The best K (with the highest score) is returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fe6020-8f23-4643-914b-5f716f1aa2ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c82a715-9e79-4f8f-ab30-207f88ca4b06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def determine_best_k_nn(X_train, y_train, k_range):\n",
    "    scores = {}\n",
    "    for k in k_range:\n",
    "        knn = KNeighborsRegressor(n_neighbors=k)\n",
    "        cv_scores = cross_val_score(knn, X_train, np.log1p(y_train), cv=5, scoring='neg_mean_squared_error')\n",
    "        scores[k] = np.mean(cv_scores)\n",
    "    best_k = max(scores, key=scores.get)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(list(scores.keys()), list(scores.values()), marker='o')\n",
    "    plt.title(\"Cross-Validated Negative MSE for Different k\")\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.ylabel(\"CV Negative MSE\")\n",
    "    plt.show()\n",
    "    return best_k, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520eeb0b-ab57-47c9-b7f7-c7b38f712d6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_knn_k, knn_scores = determine_best_k_nn(os_X_train, os_y_train, range(1, 21))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc651e7b-7a67-4568-8879-f833fee81b85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_knn_k, knn_scores = determine_best_k_nn(pfs_X_train, pfs_y_train, range(1, 21))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113c8e85-2729-405e-958c-ef0e49dabc01",
   "metadata": {},
   "source": [
    "## Full Pipeline Function\n",
    "\n",
    "The `run_pipeline` function runs the entire workflow for a given survival type (\"OS\" or \"PFS\"). It loads data, performs visualizations, evaluates models, plots predictions, and executes hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aad551-3abb-43ab-9fdb-595fbed555b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_pipeline(survival_type, optimize=False):\n",
    "    if survival_type.lower() == 'os':\n",
    "        event_col = 'hadSurvivalEvent'\n",
    "        duration_col = 'observedOsFromTreatmentStartDays'\n",
    "        label = \"Observed OS\"\n",
    "        best_k = 6\n",
    "    elif survival_type.lower() == 'pfs':\n",
    "        event_col = 'hadProgressionEvent'\n",
    "        duration_col = 'observedPfsDays'\n",
    "        label = \"Observed PFS\"\n",
    "        best_k = 7\n",
    "    else:\n",
    "        raise ValueError(\"survival_type must be 'OS' or 'PFS'\")\n",
    "    \n",
    "    df, X_train, X_test, y_train, y_test, enc_cols = get_data(query, event_col, duration_col, features, group_treatments=True)\n",
    "    \n",
    "    log_distribution = visualize_target_distribution(df, duration_col)\n",
    "    plot_correlation_heatmap(df, duration_col)\n",
    "    \n",
    "    models_dict = {\n",
    "        \"LinearRegression\": LinearRegression(),\n",
    "        \"Ridge\": Ridge(random_state=42),\n",
    "        \"Lasso\": Lasso(random_state=42),\n",
    "        \"RandomForest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        \"GradientBoosting\": GradientBoostingRegressor(random_state=42),\n",
    "        \"MLPRegressor\": MLPRegressor(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42),\n",
    "        \"SVR_RBF\": SVR(kernel='rbf', C=1.0),\n",
    "        \"XGBRegressor\": XGBRegressor(n_estimators=100, random_state=42), \n",
    "        \"KNN\": KNeighborsRegressor(n_neighbors=best_k)\n",
    "    }\n",
    " \n",
    "    results_df = evaluate_models(models_dict, X_train, X_test, y_train, y_test)\n",
    "    print(results_df)\n",
    "    \n",
    "    best_model_name = results_df.sort_values(by=\"R² (log)\", ascending=False).iloc[0][\"Model\"]\n",
    "    best_model = models_dict[best_model_name]\n",
    "    best_model.fit(X_train, np.log1p(y_train))\n",
    "    y_pred_log = best_model.predict(X_test)\n",
    "    plot_predictions(y_test, y_pred_log, f\"{best_model_name}: Predicted vs Actual {label}\")\n",
    "    plot_residuals(np.log1p(y_test), y_pred_log)\n",
    "    \n",
    "    if optimize:\n",
    "        optimize_model_random_search(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1a27d5-16dd-40eb-a277-db4ddde4b1fe",
   "metadata": {},
   "source": [
    "### Running the Pipeline\n",
    "\n",
    "Call `run_pipeline` with either `\"OS\"` or `\"PFS\"` to execute the entire workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2304ef-445b-4520-9b3d-fcf4dcb0b9f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_pipeline('OS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1636e19f-1bd3-48e0-a707-4b3ea694c8b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_pipeline('PFS')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (prediction_env)",
   "language": "python",
   "name": "prediction_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

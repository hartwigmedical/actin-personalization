{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d61ed0d-783d-45f7-99a2-f3c87dbccfc2",
   "metadata": {},
   "source": [
    "# Predictive Algorithms for Survival Analysis\n",
    "\n",
    "This notebook demonstrates the pipeline for developing and evaluating predictive algorithms in survival analysis. The primary objective is to model and predict **overall survival (OS)** and **progression-free survival (PFS)** for patients. Using both classical statistical methods and state-of-the-art deep learning techniques, the notebook covers the entire process, including:\n",
    "- Data Preprocessing: Preparing survival datasets for analysis, ensuring compatibility with various model types.\n",
    "- Model Training: Building survival models tailored to predict survival outcomes and handle censored data.\n",
    "- Hyperparameter Optimization: Fine-tuning models for optimal performance.\n",
    "\n",
    "The interpretation and evaluation of the models can be found in `predictive_algorithms_interpretation.ipynb` and contains:\n",
    "- Performance Evaluation: Comparing models based on metrics such as concordance index (C-Index), integrated Brier score (IBS), calibration error (CE), and time-dependent AUC.\n",
    "- Visualization: Generating survival curves and feature importance plots to interpret model predictions and uncover key insights.\n",
    "\n",
    "This workflow provides a framework to explore survival modeling techniques and tailor them to specific datasets and objectives.\n",
    "\n",
    "In the file `utils/settings.py` all the experiment settings can be set (e.g. OS or PFS, grouped treatments or not), then the experiment can be run in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00789d13-178f-42bd-81a8-947b666139f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bbff7b-ecbb-418e-83c5-84b5d19c1e48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"]   = \"4\"\n",
    "os.environ[\"MKL_NUM_THREADS\"]   = \"4\"\n",
    "\n",
    "torch.set_num_threads(4)\n",
    "torch.set_num_interop_threads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b78c7cb-fe63-46a0-9873-be7edde348b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir('/data/repos/actin-personalization/prediction')\n",
    "sys.path.insert(0, os.path.abspath(\"src/main/python\"))\n",
    "\n",
    "from models import *\n",
    "from utils.settings import config_settings\n",
    "from data.data_processing import DataSplitter, DataPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d87935-155b-40c6-8edc-d7c8326d2ef5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "In this section, we set up the data pipeline for survival analysis. The `DataSplitter` and `DataPreprocessor` classes from `data/data_processing.py` are used to load, preprocess, and split the data into training and testing sets. This ensures the survival data is structured appropriately for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a8049d-485b-4da4-913f-510906547b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sksurv.util import Surv\n",
    "\n",
    "def get_data():\n",
    "    preprocessor = DataPreprocessor(fit=True)\n",
    "    splitter = DataSplitter(test_size=0.1, random_state=42)\n",
    "    \n",
    "    df, features, encoded_columns = preprocessor.preprocess_data()\n",
    "    \n",
    "    if len(df[config_settings.event_col].value_counts()) == 1:\n",
    "        y = np.array([(True, t) for t in df[config_settings.duration_col]], dtype=[(config_settings.event_col, '?'), (config_settings.duration_col, 'f8')])\n",
    "    else:\n",
    "        y = Surv.from_dataframe(event=config_settings.event_col, time=config_settings.duration_col, data=df)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = splitter.split(df[features], y, encoded_columns)\n",
    "    \n",
    "    return df, X_train, X_test, y_train, y_test, encoded_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfbf72a-d031-43fe-ae55-812bb0cbef2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df, X_train, X_test, y_train, y_test, encoded_columns = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d1e5db-ed5e-4ae4-95a4-f92e0262365f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Remove after done testing (when added to json after hyperparameter tuning)\n",
    "\n",
    "configs = {\n",
    "    \"MultiTaskNN_LogisticHazard\": (\n",
    "        MultiTaskNNSurvivalModel,\n",
    "        {\n",
    "            \"model_class\": LogisticHazard,\n",
    "            \"num_nodes\": [128, 64],\n",
    "            \"num_tasks\": 10,\n",
    "            \"num_durations\": 60,\n",
    "            \"dropout\": 0.1,\n",
    "            \"epochs\": 50,\n",
    "            \"activation\": \"relu\",\n",
    "            \"batch_norm\": False,\n",
    "            \"lr\": 1e-3,\n",
    "        }\n",
    "    ),\n",
    "    \"MultiTaskNN_DeepSurv\": (\n",
    "        MultiTaskNNSurvivalModel,\n",
    "        {\n",
    "            \"model_class\": CoxPH,   # DeepSurv is CoxPH in pycox\n",
    "            \"num_nodes\": [128, 64],\n",
    "            \"num_tasks\": 10,\n",
    "            \"dropout\": 0.1,\n",
    "            \"epochs\": 100,\n",
    "            \"activation\": \"elu\",\n",
    "            \"batch_norm\": True,\n",
    "            \"lr\": 1e-3,\n",
    "            \"weight_decay\": 1e-4,\n",
    "        }\n",
    "    ),\n",
    "    \"MultiTaskNN_DeepHit\": (\n",
    "        MultiTaskNNSurvivalModel,\n",
    "        {\n",
    "            \"model_class\": DeepHit,\n",
    "            \"num_nodes\": [256, 128, 64],\n",
    "            \"num_tasks\": 10,\n",
    "            \"num_durations\": 60,\n",
    "            \"dropout\": 0.15,\n",
    "            \"epochs\": 200,\n",
    "            \"activation\": \"swish\",\n",
    "            \"alpha\": 0.2,\n",
    "            \"sigma\": 0.1,\n",
    "            \"lr\": 1e-3,\n",
    "            \"batch_norm\": False,\n",
    "        }\n",
    "    ),\n",
    "    \"MultiTaskNN_PCHazard\": (\n",
    "        MultiTaskNNSurvivalModel,\n",
    "        {\n",
    "            \"model_class\": PCHazard,\n",
    "            \"num_nodes\": [64, 32],\n",
    "            \"num_tasks\": 10,\n",
    "            \"num_durations\": 100,\n",
    "            \"dropout\": 0.1,\n",
    "            \"epochs\": 200,\n",
    "            \"activation\": \"relu\",\n",
    "            \"lr\": 1e-3,\n",
    "        }\n",
    "    ),\n",
    "    \"MultiTaskNN_MTLR\": (\n",
    "        MultiTaskNNSurvivalModel,\n",
    "        {\n",
    "            \"model_class\": MTLR,\n",
    "            \"num_nodes\": [64],\n",
    "            \"num_tasks\": 10,\n",
    "            \"dropout\": 0.2,\n",
    "            \"epochs\": 100,\n",
    "            \"activation\": \"relu\",\n",
    "            \"lr\": 5e-4,\n",
    "            \"batch_norm\": False,\n",
    "        }\n",
    "    ),\n",
    "}\n",
    "\n",
    "multi_results_df, multi_trained_models = train_evaluate_models(\n",
    "    configs,\n",
    "    max_time=1825,\n",
    "    patient_index=78,\n",
    "    selected_models=[\"MultiTaskNN_LogisticHazard\"] \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f504d62f-a330-402b-b58e-a3b3f74a9658",
   "metadata": {},
   "source": [
    "## Train and Evaluate Models\n",
    "\n",
    "This section defines the function `train_evaluate_models`, which trains various survival models using predefined configurations (as can all be found in `models/configs/...`. The trained models are evaluated using the C-index, Integrated Brier Score (IBS), Calibration Error (CE) and Area Under the Curve (AUC) as explained in `predictive_algorithms_interpretation.ipynb`. \n",
    "\n",
    "Together, these metrics provide a comprehensive evaluation of the models' predictive performance, capturing different aspects of accuracy, discrimination, and calibration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71536d40-de44-4f55-9e44-c1f1e3cc5fc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_different_models_survival_curves(trained_models, X_test, y_test, patient_index, actual_line = True):\n",
    "    \"\"\"\n",
    "    Plot survival curves for a specific patient using trained models.\n",
    "    \"\"\"\n",
    "    X_patient = X_test.iloc[[patient_index]]\n",
    "    actual_duration_days = y_test[patient_index][config_settings.duration_col]\n",
    "    actual_event = y_test[patient_index][config_settings.event_col]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for model_name, model in trained_models.items():\n",
    "        try:\n",
    "            surv_funcs = model.predict_survival_function(X_patient)\n",
    "\n",
    "            times = np.linspace( max(fn.x[0] for fn in surv_funcs), min(fn.x[-1] for fn in surv_funcs), 100)\n",
    "            surv_probs = np.row_stack([fn(times) for fn in surv_funcs])\n",
    "            \n",
    "            plt.step(times / 30.44, surv_probs[0], where=\"post\", label=model_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting survival curves for model {model_name}: {e}\")\n",
    "\n",
    "    if actual_line:\n",
    "        marker_color = 'red' if actual_event else 'blue'\n",
    "        marker_label = \"Event Time\" if actual_event else \"Censoring Time\"\n",
    "        \n",
    "        plt.axvline(x=actual_duration_days / 30.44, color=marker_color, linestyle='--', label=marker_label)\n",
    "\n",
    "    plt.title(f\"Predicted {config_settings.outcome} Curves\")\n",
    "    plt.xlabel(\"Time (months)\")\n",
    "    plt.ylabel(\"Survival Probability\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9317cc-59e1-430f-8e93-d3e10257d308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "import importlib\n",
    "\n",
    "def save_model_output(results_df):\n",
    "    os.makedirs(config_settings.save_path, exist_ok=True)\n",
    "\n",
    "    csv_file = os.path.join(config_settings.save_path, f\"{config_settings.outcome}_model_outcomes.csv\")\n",
    "    if os.path.exists(csv_file):\n",
    "        existing_df = pd.read_csv(csv_file)\n",
    "        merged_df = pd.concat([existing_df, results_df]).drop_duplicates(\n",
    "            subset=['Model'], keep='last'\n",
    "        )\n",
    "        merged_df.to_csv(csv_file, index=False)\n",
    "        print(f\"Updated model outcomes saved to {csv_file}\")\n",
    "    else:\n",
    "        results_df.to_csv(csv_file, index=False)\n",
    "        print(f\"Model outcomes saved to {csv_file}\")\n",
    "\n",
    "def train_evaluate_models(configs, max_time=1825, patient_index = 78, selected_models = None):\n",
    "    \n",
    "    df, X_train, X_test, y_train, y_test, encoded_columns = get_data()\n",
    "    if config_settings.save_models:\n",
    "        with open(f\"{config_settings.save_path}/{config_settings.outcome}_preprocessor/label_encodings.json\", \"w\") as f:\n",
    "            json.dump(encoded_columns, f)\n",
    "    \n",
    "    if selected_models is not None:\n",
    "        configs = {k: v for k, v in configs.items() if k in selected_models}\n",
    "        if not configs:\n",
    "            raise ValueError(f\"No matching models found in configs for: {selected_models}\")\n",
    "\n",
    "    models = {}        \n",
    "    for model_name, (model_class, model_kwargs) in configs.items():\n",
    "        if \"input_size\" not in model_kwargs and (\"input_size\" in model_class.__init__.__code__.co_varnames):\n",
    "            model_kwargs['input_size'] = X_train.shape[1]\n",
    "        models[model_name] = model_class(**model_kwargs)\n",
    "    \n",
    "    trainer = ModelTrainer(models=models)\n",
    "    \n",
    "    results, trained_models = trainer.train_and_evaluate(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        encoded_columns=encoded_columns,\n",
    "    )\n",
    "    \n",
    "    results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "    results_df.reset_index(inplace=True)\n",
    "    results_df.rename(columns={'index': 'Model'}, inplace=True)\n",
    "    \n",
    "#     if config_settings.save_models:\n",
    "#         save_model_output(results_df)\n",
    "#         best_models = {}\n",
    "#         for model_name, (model_class, model_kwargs) in configs.items():\n",
    "#             # grab the *trained* model instance\n",
    "#             trained = trained_models[model_name]\n",
    "#             # these are the hyperparams you want to persist\n",
    "#             best_models[model_name] = (trained, model_kwargs)\n",
    "#         ExperimentConfig.update_model_hyperparams(best_models)\n",
    "\n",
    "        \n",
    "#     plot_different_models_survival_curves(trained_models, X_test, y_test, patient_index)\n",
    "    \n",
    "    return results_df, trained_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cfc2de-8126-4ccc-ae60-b846528673a4",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization\n",
    "\n",
    "Hyperparameter optimization is performed for each model using a defined grid of parameters. The `random_parameter_search` function samples configurations to identify the optimal parameters for each model (can be found in `models/hyperparameter_optimization`). This ensures that models achieve their best performance for the given data.\n",
    "\n",
    "After optimization the models, including results are stored in `models/trained_models`. The optimal configurations are stored in `models/configs/model_hyperparams.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c208e0c-00eb-47dd-a74b-bf3e021cb4da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def optimize_hyperparameters():\n",
    "    \n",
    "    df, X_train, X_test, y_train, y_test, encoded_columns = get_data()\n",
    "          \n",
    "    models = {\n",
    "        'DeepSurv': DeepSurv(input_size=X_train.shape[1], use_attention=False),\n",
    "        'DeepSurv_attention': DeepSurv(input_size=X_train.shape[1], use_attention=True),\n",
    "        \n",
    "        'LogisticHazardModel': LogisticHazardModel(input_size=X_train.shape[1], use_attention=False),\n",
    "        'LogisticHazardModel_attention': LogisticHazardModel(input_size=X_train.shape[1], use_attention=True),\n",
    " \n",
    "        'DeepHitModel': DeepHitModel(input_size=X_train.shape[1], use_attention=False),\n",
    "        'DeepHitModel_attention': DeepHitModel(input_size=X_train.shape[1], use_attention=True),\n",
    "        \n",
    "        'PCHazardModel': PCHazardModel(input_size=X_train.shape[1], use_attention=False), \n",
    "        'PCHazardModel_attention': PCHazardModel(input_size=X_train.shape[1], use_attention=True),\n",
    "        \n",
    "        'MTLRModel': MTLRModel(input_size=X_train.shape[1], use_attention=False),\n",
    "        'MTLRModel_attention': MTLRModel(input_size=X_train.shape[1], use_attention=True),\n",
    "\n",
    "        'CoxPH': CoxPHModel(),\n",
    "        'RandomSurvivalForest': RandomSurvivalForestModel(),\n",
    "        'GradientBoosting': GradientBoostingSurvivalModel(),\n",
    "    }\n",
    "    \n",
    "    best_models, all_results = hyperparameter_search(\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        encoded_columns=encoded_columns,\n",
    "        base_models=models, param_grids=curve_param_grids)\n",
    "           \n",
    "    return best_models, all_results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf478771-030d-41ed-a26e-f91ac670995f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_models, results = optimize_hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32032079-2e45-443b-ad67-1721f39c2c4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Best Model Configurations\n",
    "\n",
    "The configurations which were determined best can be used to instantiate the models for training and evaluation. The best configurations are stored in `models/configs/model_hyperparams.json`.\n",
    "\n",
    "Once you've trained or updated the models locally, you can upload the entire prediction/trained models back to the bucket with:\n",
    "\n",
    "`gsutil -m rsync -r -x \".*/\\.ipynb_checkpoints/.*\" /data/patient_like_me/prediction/trained_models/ gs://actin-personalization-models-v1/trained_models/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaa668e-6921-4cb4-bb7c-ec89e8dd902a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = ExperimentConfig(config_settings.json_config_file)\n",
    "configs = config.load_model_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35973037-bcc6-4556-a0b7-93c5a2e02c67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_outcomes, trained_models =  train_evaluate_models(configs=configs, selected_models = [\"DeepSurv_attention\"])\n",
    "model_outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cbb53f-7bfa-4c9f-bf05-3f0582e543b8",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "Explicit feature selection is applied to CoxPH model to improve interpretability and reduce noise:\n",
    "\n",
    "- `CoxPH`: \n",
    "    - Features with high p-values (non-significant) are removed.\n",
    "    - Multicollinearity is addressed by excluding highly correlated predictors.\n",
    "    \n",
    "Other models, such as tree-based or neural survival models, inherently manage feature selection through their architecture or regularization techniques, making explicit feature filtering unnecessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06dae5a-17b2-4640-83a2-0fc9a02073f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_select_coxph(model, X_train, threshold=0.01):\n",
    "    \"\"\"\n",
    "    For CoxPH: Remove features with abs(coef) < threshold.\n",
    "    \"\"\"\n",
    "    if hasattr(model.model, 'coef_'):\n",
    "        coefs = model.model.coef_\n",
    "        feature_mask = np.abs(coefs) > threshold\n",
    "        retained = model.selected_features[feature_mask]\n",
    "        if len(retained) == 0:\n",
    "            retained = model.selected_features\n",
    "        return retained\n",
    "    else:\n",
    "        return model.selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f18ce2e-be3f-478a-8891-0eb448609bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refit_model_with_selected_features(model_name, original_model, X_train, y_train, X_test, y_test, retained_features):\n",
    "    \n",
    "    retained_features = [f for f in retained_features if f != \"Intercept\"]\n",
    "\n",
    "    y_train_df = pd.DataFrame({'duration': y_train[config_settings.duration_col], 'event': y_train[config_settings.event_col]}, index=X_train.index)\n",
    "    y_train_structured = Surv.from_dataframe('event', 'duration', y_train_df)\n",
    "\n",
    "    y_test_df = pd.DataFrame({'duration': y_test[config_settings.duration_col], 'event': y_test[config_settings.event_col]}, index=y_test.index)\n",
    "    y_test_structured = Surv.from_dataframe('event', 'duration', y_test_df)\n",
    "\n",
    "    model_class = type(original_model)\n",
    "    model_kwargs = getattr(original_model, 'kwargs', {})\n",
    "    new_model = model_class(**model_kwargs)\n",
    "    new_model.fit(X_train[retained_features], y_train_structured)\n",
    "\n",
    "    trainer = ModelTrainer(models={}, n_splits=5, random_state=42, max_time=config_settings.max_time)\n",
    "    holdout_metrics = trainer._evaluate_model(\n",
    "        new_model,\n",
    "        X_test[retained_features],\n",
    "        y_train_structured,\n",
    "        y_test_structured,\n",
    "        y_test_df,\n",
    "        model_name\n",
    "    )\n",
    "    print(f\"{model_name} Feature-Selected Hold-Out Results: {holdout_metrics}\")\n",
    "\n",
    "    if save_models:\n",
    "        save_new_model(new_model, model_name)\n",
    "\n",
    "    return new_model\n",
    "\n",
    "def save_new_model(model, model_name, suffix=\"_feature_selected\"):\n",
    "    new_model_name = model_name + suffix\n",
    "    model_file = os.path.join(config_settings.save_path, f\"{config_settings.outcome}_{new_model_name}\")\n",
    "    with open(model_file + \".pkl\", \"wb\") as f:\n",
    "        dill.dump(model, f)\n",
    "    print(f\"New model with feature selection saved as {config_settings.outcome}_{new_model_name}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e902a0c8-8c1b-477c-b57a-1d3fe9c90144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_and_refit(X_train, y_train, X_test, y_test):\n",
    "    coxph_model = load_trained_model(\"CoxPH\", CoxPHModel, model_kwargs=configs['CoxPH'][1])\n",
    "    aalen_model = load_trained_model(\"AalenAdditive\", AalenAdditiveModel, model_kwargs=configs['AalenAdditive'][1])\n",
    "    \n",
    "    coxph_retained = feature_select_coxph(coxph_model, X_train, threshold=0.01)\n",
    "    aalen_retained = feature_select_aalen_additive(aalen_model, X_train, threshold=0.001)\n",
    "\n",
    "    new_coxph_model = refit_model_with_selected_features(\"CoxPH\", coxph_model, X_train, y_train, X_test, y_test, coxph_retained)\n",
    "    new_aalen_model = refit_model_with_selected_features(\"AalenAdditive\", aalen_model, X_train, y_train, X_test, y_test, aalen_retained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8bd144-e197-4d2a-82de-da1984d82fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "select_features_and_refit(X_train, y_train, X_test, y_test, configs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (prediction)",
   "language": "python",
   "name": "prediction_py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
